{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9251ab6",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe4f9d1",
   "metadata": {},
   "source": [
    "Say that we have a node $i$ connected to node $c$. The question which arises is: how much the features of node $c$ are important to node $i$, and can such importance be learnt automatically?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc42148",
   "metadata": {},
   "source": [
    "# 2. Graph Attention Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910c2b89",
   "metadata": {},
   "source": [
    "**Input:** Set of node features $\\mathbf{h} = \\{\\bar{h}_1, \\bar{h}_2, \\cdots, \\bar{h}_n\\}$ where $\\bar{h}_i \\in \\mathbb{R}^F$<br>\n",
    "**Input:** New set of node features $\\mathbf{h'} = \\{\\bar{h'}_1, \\bar{h'}_2, \\cdots, \\bar{h'}_n\\}$ where $\\bar{h'}_i \\in \\mathbb{R}^{F'}$<br>\n",
    "\n",
    "1. Apply a **parameterized linear transformation** to every node: $\\mathbf{W}\\cdot \\bar{h}_i$ where $\\mathbf{W} \\in \\mathbb{R}^{F' \\times F}$\n",
    "2. Apply **self attention**: $a: \\mathbb{R}^{F'} \\times \\mathbb{R}^{F'} \\to \\mathbb{R}$. Applying this we get the result <br>\n",
    "   $$e_{i,j} = a(\\mathbf{W}\\cdot \\bar{h}_i, \\mathbf{W}\\cdot \\bar{h}_j) $$\n",
    "3. **Normalization**: $$\\alpha_{i,j} = softmax_j (e_{i,j}) = \\dfrac{\\exp(e_{i,j})}{\\sum_{k\\in\\mathcal{N}(i)}\\exp(e_{i,k})}$$\n",
    "4. **Attention mechanism**: $a$ is a single layer feedforward neural network. The two vectors $\\mathbf{W}\\cdot h_i \\to h_i'$ and $\\mathbf{W}\\cdot h_j \\to h_j'$ are concatenated to get $\\bar{a} \\in \\mathbb{R}^{2F'}$ and this is passed to the Leaky ReLU function, i.e $\\max(0.2x, x)$. Finally this get passed to the $softmax_j$ step.\n",
    "\n",
    "Finally we can say\n",
    "$$\\alpha_{i,j} = \\dfrac{\\exp(LeakyReLU(\\bar{a}^T [\\mathbf{W}\\cdot h_i || \\mathbf{W}\\cdot h_j]))}{\\sum_{k \\in \\mathcal{N}(i)} \\exp(LeakyReLU(\\bar{a}^T [\\mathbf{W}\\cdot h_i || \\mathbf{W}\\cdot h_k]))} $$\n",
    "\n",
    "5. **Message passing**: $h_i' = \\sigma(\\sum_{j \\in \\mathcal{N}(i)} \\alpha_{i,j}\\mathbf{W}h_j)$\n",
    "6. **Multi-head attention**: We repeat the procedure several times. Thus we have can either concatenate the results (say we run it $K$ times), or we can average it. The authors suggest to concatenate in the internal layers and average in the final layer of the network.\n",
    "    <br>Concatenation: $$h_i' = ||_{k=1}^K \\sigma(\\sum_{j \\in \\mathcal{N}(i)} \\alpha_{i,j}^k\\mathbf{W}^kh_j)$$\n",
    "Average: $$h_i' = \\sigma\\bigg(\\dfrac{1}{K} \\sum_{k=1}^K \\sum_{j \\in \\mathcal{N}(i)} \\alpha_{i,j}^k\\mathbf{W}^kh_j \\bigg)$$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985c17cf",
   "metadata": {},
   "source": [
    "# 3. Advantages of GAT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb04aec",
   "metadata": {},
   "source": [
    "1. Computationally efficient since we can parallelize the self-attention layers across edges and the output features can be parellelized across nodes.\n",
    "2. Allows to assign different importance to nodes of a same neighborhood\n",
    "3. It is applied in a shared manner to all edges in the graph, and thus we don't need to have the entire graph.\n",
    "4. Works in both: transductive and inductive learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a53f452",
   "metadata": {},
   "source": [
    "# 4. Message Passing Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86180c40",
   "metadata": {},
   "source": [
    "From PyTorch Geometric, we can write that \n",
    "$$ \\mathbf{x}_i^{(k)} = \\gamma^{(k)} \\bigg(\\mathbf{x}_i^{(k-1)}, \\square_{j \\in \\mathcal{N}(i)} \\phi^{(k)} \\Big(\\mathbf{x}_i^{(k-1)},\\mathbf{x}_j^{(k-1)}, \\mathbf{e}_{j, i}\\Big)\\bigg) $$\n",
    "where \n",
    "1. $\\mathbf{x}_i^{(k)}$ are the feature representations of node $i$ in the $k^{th}$ layer\n",
    "2. $\\mathbf{x}_i^{(k-1)},\\mathbf{x}_j^{(k-1)}, \\mathbf{e}_{j, i}$ are the feature representations in the $(k-1)^{th}$ layer, and optionally we have edge features of the edge $i-j$\n",
    "3. $\\phi^{(k)}$ is a differentiable function such as MLP\n",
    "4. $\\square$ is a differentiable ordering invariant function\n",
    "5. $\\gamma^{(k)}$ is another differentiable function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36182ce6",
   "metadata": {},
   "source": [
    "# 5. Practice the MessagePassing class in PyTorch Geometric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48e7f93",
   "metadata": {},
   "source": [
    "The GCNConv layer has the following implementation:\n",
    "$$\\mathbf{x}_i^{(k)} = \\sum_{j \\in \\mathcal{N}(i) \\cup {i}} \\dfrac{1}{\\sqrt{deg(i)}\\sqrt{deg(j)}} \\cdot \\Big(\\mathbf{\\Theta}\\cdot \\mathbf{x}_j^{(k-1)}\\Big) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a774f3a",
   "metadata": {},
   "source": [
    "The $\\phi$ here becomes the parameter $\\mathbf{\\Theta}$ and the $\\square$ becomes the sum of the degree normalization term.<br>\n",
    "In order to implement this in PyTorch, we need to do the following steps:\n",
    "1. Add self loops\n",
    "2. A linear transformation to node feature matrix\n",
    "3. Compute normalization coefficients\n",
    "4. Normalize node features\n",
    "5. Sum up neighboring node features\n",
    "The first three steps are done in the forward method, the 4th step in the message method and the sum is done in the initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d56b19af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops, degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2528ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNConv(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GCNConv, self).__init__(aggr='add')\n",
    "        self.lin = torch.nn.Linear(in_channels, out_channels)\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        # x.shape = (N, in_channels)\n",
    "        # edge_index.shape = (2, E)\n",
    "        \n",
    "        # 1. Add self loops\n",
    "        edge_index, _ = add_self_loops(edge_index, numn_nodes = x.size(0))\n",
    "        \n",
    "        #2. Linear transformation\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        #3. Normalization\n",
    "        row, col = edge_index\n",
    "        deg = degree(col, x.size(0), dtype=x.dtype)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "        \n",
    "        #4/5.Start propogating messages\n",
    "        return self.propogate(edge_index, x=x, norm=norm)\n",
    "    \n",
    "    def message(self, x_j, norm):\n",
    "        #x_j.shape = (E, out_channels)\n",
    "        return norm.view(-1, 1) * x_j\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7929165",
   "metadata": {},
   "source": [
    "# 6. Implement GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61f767ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "420e6eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GATLayer, self).__init__()\n",
    "        \n",
    "    def forward(self, input, adj):\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2b5b85",
   "metadata": {},
   "source": [
    "## Linear Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839f110f",
   "metadata": {},
   "source": [
    "$$ \\bar{h}_i' = \\mathbf{W}\\bar{h}_i$$\n",
    "As before $\\mathbf{W} \\in \\mathbb{R}^{F' \\times F}$ and $\\bar{h}_i \\in \\mathbb{R}^F$, thus $\\bar{h}_i' \\in \\mathbf{R}^{F'}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4dce512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "in_features = 5\n",
    "out_features = 2\n",
    "nb_nodes = 3\n",
    "\n",
    "W = nn.Parameter(torch.zeros(size=(in_features, out_features))) # xavier parameter initialization\n",
    "nn.init.xavier_uniform_(W.data, gain=1.414)\n",
    "input = torch.rand(nb_nodes, in_features)\n",
    "\n",
    "h = torch.mm(input, W)\n",
    "N = h.size()[0]\n",
    "print(h.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7eb2b6b",
   "metadata": {},
   "source": [
    "## Attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52576ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1])\n"
     ]
    }
   ],
   "source": [
    "a = nn.Parameter(torch.zeros(size=(2*out_features, 1)))\n",
    "nn.init.xavier_uniform_(a.data, gain=1.414)\n",
    "print(a.shape)\n",
    "\n",
    "leaky_relu = nn.LeakyReLU(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef7ebf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_input = torch.cat([h.repeat(1, N).view(N * N, -1), h.repeat(N, 1)], dim=1).view(N, -1, 2 * out_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53c9d8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = leaky_relu(torch.matmul(a_input, a).squeeze(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83d2de77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3, 4]) torch.Size([4, 1])\n",
      "\n",
      "torch.Size([3, 3, 1])\n",
      "\n",
      "torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "print(a_input.shape, a.shape)\n",
    "print()\n",
    "print(torch.matmul(a_input, a).shape)\n",
    "print()\n",
    "print(torch.matmul(a_input, a).squeeze(2).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20b46c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0142, -0.0059,  0.0037],\n",
      "        [ 0.0908,  0.1327,  0.1658],\n",
      "        [ 0.2506,  0.2925,  0.3256]], grad_fn=<LeakyReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e7cf42",
   "metadata": {},
   "source": [
    "## Masked attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be972e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3])\n",
      "tensor([[-9.0000e+15, -9.0000e+15, -9.0000e+15],\n",
      "        [-9.0000e+15, -9.0000e+15, -9.0000e+15],\n",
      "        [-9.0000e+15, -9.0000e+15, -9.0000e+15]])\n"
     ]
    }
   ],
   "source": [
    "adj = torch.randint(2, (3, 3))\n",
    "zero_vec = -9e15 * torch.ones_like(e)\n",
    "print(zero_vec.shape)\n",
    "print(zero_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97c9913c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 0, 0],\n",
      "        [1, 1, 0],\n",
      "        [0, 1, 0]])\n",
      "tensor([[-0.0142, -0.0059,  0.0037],\n",
      "        [ 0.0908,  0.1327,  0.1658],\n",
      "        [ 0.2506,  0.2925,  0.3256]], grad_fn=<LeakyReluBackward0>)\n",
      "tensor([[-9.0000e+15, -9.0000e+15, -9.0000e+15],\n",
      "        [-9.0000e+15, -9.0000e+15, -9.0000e+15],\n",
      "        [-9.0000e+15, -9.0000e+15, -9.0000e+15]])\n",
      "tensor([[-1.4248e-02, -9.0000e+15, -9.0000e+15],\n",
      "        [ 9.0823e-02,  1.3270e-01, -9.0000e+15],\n",
      "        [-9.0000e+15,  2.9252e-01, -9.0000e+15]], grad_fn=<SWhereBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attention = torch.where(adj > 0, e, zero_vec)\n",
    "print(adj)\n",
    "print(e)\n",
    "print(zero_vec)\n",
    "print(attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7eeb2078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.4895, 0.5105, 0.0000],\n",
      "        [0.0000, 1.0000, 0.0000]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[ 0.2003,  0.1686],\n",
      "        [-0.0097,  0.0547],\n",
      "        [-0.2111, -0.0546]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attention = F.softmax(attention, dim=1)\n",
    "h_prime = torch.matmul(attention, h)\n",
    "print(attention)\n",
    "print(h_prime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6779d86e",
   "metadata": {},
   "source": [
    "## Finalizing the GAT layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee125ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
    "        super(GATLayer, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha\n",
    "        self.concat = concat\n",
    "        \n",
    "        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
    "        \n",
    "        self.a = nn.Parameter(torch.zeros(size=(2 * out_features, 1)))\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "        \n",
    "        self.leaky_relu = nn.LeakyReLU(self.alpha)\n",
    "        \n",
    "    def forward(self, input, adj):\n",
    "        # 1. Linear transformation\n",
    "        h = torch.mm(input, self.W)\n",
    "        N = h.size()[0]\n",
    "        \n",
    "        # 2. Attention mechanism\n",
    "        a_input = torch.cat([h.repeat(1, N).view(N * N, -1), h.repeat(N, 1)], dim=1).view(N, -1, 2 * self.out_features)\n",
    "        e = self.leaky_relu(torch.matmul(a_input, self.a).squeeze(2))\n",
    "        \n",
    "        # 3. Masked attention\n",
    "        zero_vec = -9e15 * torch.ones_like(e)\n",
    "        attention = torch.where(adj > 0, e, zero_vec)\n",
    "        attention = F.softmax(attention, dim=1)\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
    "        h_prime = torch.matmul(attention, h)\n",
    "        if self.concat:\n",
    "            return F.elu(h_prime)\n",
    "        else:\n",
    "            return h_prime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c6ebfc",
   "metadata": {},
   "source": [
    "## Usage of the implementation in PyTorch Geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3e7bcfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 1433\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dataset = Planetoid(root='Cora_dataset', name='Cora')\n",
    "dataset.transform = T.NormalizeFeatures()\n",
    "print(dataset.num_classes, dataset.num_node_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9354d5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GAT, self).__init__()\n",
    "        self.hid = 8\n",
    "        self.in_head = 8\n",
    "        self.out_head = 1\n",
    "        \n",
    "        self.conv1 = GATConv(dataset.num_features, self.hid, heads=self.in_head, dropout=0.6)\n",
    "        self.conv2 = GATConv(self.hid*self.in_head, dataset.num_classes, concat=False, heads=self.out_head, dropout=0.6)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "42c6f3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0f96896e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GAT().to(device)\n",
    "data = dataset[0].to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "833b5d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.9461, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6521, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5752, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5749, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5524, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for e in range(1000):\n",
    "    model.train()\n",
    "    opt.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    \n",
    "    if e % 200 == 0:\n",
    "        print(loss)\n",
    "        \n",
    "    loss.backward()\n",
    "    opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6938c86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8210\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "_, pred = model(data).max(dim=1)\n",
    "correct = float(pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\n",
    "acc = correct/data.test_mask.sum().item()\n",
    "print(\"Accuracy: {:.4f}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a0c208",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
